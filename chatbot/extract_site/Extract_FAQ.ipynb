{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What Bachelor’s programmes are offered by the UvA?\n",
      "<div class=\"faq-answer\">\n",
      " <p>\n",
      "  An overview of all our Bachelor’s programmes can be found online. On each degree programme page, you will find information about the relevant degree programme, admission requirements, deadlines and contact details.\n",
      " </p>\n",
      " <ul>\n",
      "  <li>\n",
      "   <a href=\"http://www.uva.nl/en/education/bachelor-s/bachelors.html\">\n",
      "    Bachelor’s programmes\n",
      "   </a>\n",
      "   <br/>\n",
      "  </li>\n",
      " </ul>\n",
      "</div>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "#from progressbar import *\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Example of question and answer (answer is html)\n",
    "\n",
    "examplePage = requests.get(\"http://student.uva.nl/en/faq/application-and-admission/application/application.html\")\n",
    "\n",
    "exampleSoup = BeautifulSoup(examplePage.content, 'html.parser')\n",
    "\n",
    "for faqItem in exampleSoup.find_all('div', class_='faq-item'):\n",
    "    \n",
    "    # plain text\n",
    "    question = faqItem.find_all('a', class_='faq-question')[0].get_text()\n",
    "    print(question)\n",
    "    \n",
    "    # html\n",
    "    answer = faqItem.find_all('div', class_='faq-answer')[0]\n",
    "    print(answer.prettify())\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITH PROGRESSBAR\n",
    "\n",
    "# Using depth-bounded depth-first search for finding urls with FAQs\n",
    "# (not really object-oriented)\n",
    "\n",
    "# A nice progress bar, since this will take a while.\n",
    "# Does require global variables ad makes the code \n",
    "# less nice in other ways as well.\n",
    "#widgets = ['Test: ', Percentage(), ' ', Bar(marker='0',left='[',right=']'),\n",
    " #      ' ', ETA()] \n",
    "\n",
    "def faq_list_generator_pb(startPage, depth, pagesRegex):\n",
    "    # so you don't do pages twice\n",
    "    crawled = set()\n",
    "    #global pbar\n",
    "    #pbar.start()\n",
    "    \n",
    "    faqList = depth_first_urls_pb(startPage, depth, crawled, pagesRegex)\n",
    "    \n",
    "    #pbar.finish()\n",
    "    return faqList\n",
    "\n",
    "def depth_first_urls_pb(currentPage, depth, crawled, pagesRegex):\n",
    "    faqList = []\n",
    "    \n",
    "    currentSoup = BeautifulSoup(currentPage.content, 'html.parser')\n",
    "    \n",
    "    # no need to add anything if it's already added, but we still might need what's underneath\n",
    "    if currentPage.url not in crawled:\n",
    "        # construct list for current page\n",
    "        for faqItem in currentSoup.find_all('div', class_='faq-item'):\n",
    "            faqList.append((faqItem, currentPage.url))\n",
    "    \n",
    "    crawled.add(currentPage.url)\n",
    "        \n",
    "    # continue deeper\n",
    "    if depth > 0:\n",
    "        # for each page under this page\n",
    "        for i, soupTag in enumerate(currentSoup.findAll('a', attrs={'href': re.compile(pagesRegex)})):\n",
    "           #global maxDepth\n",
    "            #if depth == maxDepth:\n",
    "                #global pbar\n",
    "                #pbar.update(i)\n",
    "            link = soupTag.get('href')\n",
    "            if link[0] == '/':\n",
    "                if \"student.uva.nl\" in currentPage.url:\n",
    "                    link = \"http://student.uva.nl\" + link\n",
    "                elif \"uva.nl\" in currentPage.url:\n",
    "                    link = \"http://www.uva.nl\" + link\n",
    "                    \n",
    "            nextPage = None\n",
    "            try:\n",
    "                nextPage = requests.get(link)\n",
    "            except:\n",
    "                print(\"This link doesn't look right:\")\n",
    "                print(link)\n",
    "                continue\n",
    "                \n",
    "            # add whatever comes out of this link and what's below it\n",
    "            faqList = faqList + depth_first_urls_pb(nextPage, depth - 1, crawled, pagesRegex)\n",
    "                \n",
    "    return faqList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# WITHOUT PROGRESSBAR\n",
    "\n",
    "# Using depth-bounded depth-first search for finding urls with FAQs\n",
    "# (not really object-oriented)\n",
    "\n",
    "# Change according to current search criteria\n",
    "# pagesRegex = \"^http://www.student.uva.nl|^http://student.uva.nl|^http://www.uva.nl|^http://uva.nl|^/\"\n",
    "pagesRegex = \"^/\"\n",
    "\n",
    "def faq_list_generator(startPage, depth, pagesRegex):\n",
    "    # so you don't do pages twice\n",
    "    crawled = set()\n",
    "    faqList = depth_first_urls(startPage, depth, crawled, pagesRegex)\n",
    "    return faqList\n",
    "\n",
    "def depth_first_urls(currentPage, depth, crawled, pagesRegex):\n",
    "    faqList = []\n",
    "    \n",
    "    currentSoup = BeautifulSoup(currentPage.content, 'html.parser')\n",
    "    \n",
    "    # no need to add anything if it's already added, but we still might need what's underneath\n",
    "    if currentPage.url not in crawled:\n",
    "        # construct list for current page\n",
    "        for faqItem in currentSoup.find_all('div', class_='faq-item'):\n",
    "            faqList.append((faqItem, currentPage.url))\n",
    "    \n",
    "    crawled.add(currentPage.url)\n",
    "        \n",
    "    # continue deeper\n",
    "    if depth > 0:\n",
    "        # for each page under this page\n",
    "        for soupTag in currentSoup.findAll('a', attrs={'href': re.compile(pagesRegex)}):\n",
    "            link = soupTag.get('href')\n",
    "            if link[0] == '/':\n",
    "                if \"student.uva.nl\" in currentPage.url:\n",
    "                    link = \"http://student.uva.nl\" + link\n",
    "                elif \"uva.nl\" in currentPage.url:\n",
    "                    link = \"http://www.uva.nl\" + link\n",
    "                    \n",
    "            nextPage = None\n",
    "            try:\n",
    "                nextPage = requests.get(link)\n",
    "            except:\n",
    "                print(\"This link doesn't look right:\")\n",
    "                print(link)\n",
    "                continue\n",
    "                \n",
    "            # add whatever comes out of this link and what's below it\n",
    "            faqList = faqList + depth_first_urls(nextPage, depth - 1, crawled, pagesRegex)\n",
    "                \n",
    "    return faqList\n",
    "\n",
    "# CHANGE according to your needs\n",
    "depth = 2\n",
    "startPage = requests.get(\"http://student.uva.nl/veelgestelde-vragen/veelgestelde-vragen.html\")\n",
    "pagesRegex = \"^/\"\n",
    "\n",
    "faqList = faq_list_generator(startPage, depth, pagesRegex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE according to your needs\n",
    "maxDepth = 3\n",
    "# startPage = requests.get(\"http://student.uva.nl/veelgestelde-vragen/veelgestelde-vragen.html\")\n",
    "\n",
    "startPage = requests.get(\"http://student.uva.nl/veelgestelde-vragen/veelgestelde-vragen.html\")\n",
    "# you can increase tree width by adding more options for the start of the url\n",
    "pagesRegex = \"^/\"\n",
    "\n",
    "\n",
    "faqList = faq_list_generator_pb(startPage, maxDepth, pagesRegex)\n",
    "# OR without progressbar\n",
    "# faqList = faq_list_generator(startPage, maxDepth, pagesRegex)\n",
    "\n",
    "\n",
    "# ADD MORE IF YOU WANT\n",
    "# Note that maxDepth is 1 if you just want the page itself\n",
    "# faqList = faqList + faq_list_generator(startPage2, maxDepth2, pagesRegex2)\n",
    "# etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# startPage = requests.get(\"http://www.uva.nl/home\")\n",
    "# startPage = requests.get(\"http://student.uva.nl/veelgestelde-vragen/ict-en-faciliteiten/ict-en-faciliteiten.html\")\n",
    "# startPage = requests.get(\"http://student.uva.nl/veelgestelde-vragen/ict-en-faciliteiten/blackboard/blackboard.html\")\n",
    "# startPage = requests.get(\"http://student.uva.nl/content/az/blackboard/veelgestelde-vragen/veelgestelde-vragen.html\")\n",
    "# pagesRegex = \"^http://www.student.uva.nl|^http://student.uva.nl|^http://www.uva.nl|^http://uva.nl|^/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "stopwords = {}\n",
    "dutch_stopwords = set()\n",
    "with open('../stopwoorden.txt') as f:\n",
    "    for line in f:\n",
    "        dutch_stopwords.add(line)\n",
    "nltk_dutch = set(nltk.corpus.stopwords.words('dutch'))\n",
    "stopwords['dutch'] = dutch_stopwords.union(nltk_dutch)\n",
    "stopwords['english'] = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def generate_keywords(sentence, lang=(1, \"English\")):\n",
    "    tokens = {'NOUN', 'ADJ', 'NUM'}\n",
    "    l = lang[1].lower()\n",
    "    sentence = sentence.lower()\n",
    "    tokenized = nltk.word_tokenize(sentence)\n",
    "    tagged = nltk.pos_tag([x for x in tokenized if x not in stopwords[l]],\n",
    "                          tagset='universal', lang=l)\n",
    "    return  ','.join(str(s) for s in set([word for (word, token) in tagged if token in tokens]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177\n"
     ]
    }
   ],
   "source": [
    "qAndA = {}\n",
    "for (x, _) in faqList:\n",
    "    question = x.find_all('a', class_='faq-question')[0].get_text()\n",
    "    answer = str(x.find_all('div', class_='faq-answer')[0])\n",
    "    #qAndA[generate_keywords(question)] = (question, answer)\n",
    "    qAndA[question] = answer\n",
    "    \n",
    "# Possible extension:\n",
    "# It's also possible to use the urls in faqList, e.g.\n",
    "# for (_, url) in faqList:\n",
    "\n",
    "# And now you can use this dictionary of questions and answers\n",
    "print(len(qAndA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORTING\n",
    "with open('export_faq_nl_nokw.txt', 'w') as file:\n",
    "     file.write(json.dumps(qAndA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
